{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":12944278,"datasetId":8191506,"databundleVersionId":13600906},{"sourceType":"datasetVersion","sourceId":12944491,"datasetId":8191667,"databundleVersionId":13601157},{"sourceType":"datasetVersion","sourceId":12908448,"datasetId":8167811,"databundleVersionId":13559467},{"sourceType":"datasetVersion","sourceId":12908574,"datasetId":8167880,"databundleVersionId":13559615},{"sourceType":"datasetVersion","sourceId":12945532,"datasetId":8191590,"databundleVersionId":13602401}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ultralytics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T04:44:57.681247Z","iopub.execute_input":"2025-09-03T04:44:57.681729Z","iopub.status.idle":"2025-09-03T04:46:20.069121Z","shell.execute_reply.started":"2025-09-03T04:44:57.681709Z","shell.execute_reply":"2025-09-03T04:46:20.068445Z"}},"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.3.191-py3-none-any.whl.metadata (37 kB)\nRequirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.2)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.4)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: polars in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.21.0)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.16-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.0->ultralytics) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nDownloading ultralytics-8.3.191-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.16-py3-none-any.whl (28 kB)\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.191 ultralytics-thop-2.0.16\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport cv2\nimport yaml\nimport pickle\nimport shutil\nimport random\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom ultralytics import YOLO\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f\"Random seed fixed at {seed}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T04:46:20.070788Z","iopub.execute_input":"2025-09-03T04:46:20.071081Z","iopub.status.idle":"2025-09-03T04:46:22.204838Z","shell.execute_reply.started":"2025-09-03T04:46:20.071058Z","shell.execute_reply":"2025-09-03T04:46:22.204303Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# ID Bound","metadata":{}},{"cell_type":"markdown","source":"## Loading Data","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport yaml\nimport pickle\nimport shutil\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom ultralytics import YOLO\n\n\ndef load_label_file(label_path: Path):\n    if not label_path.exists():\n        return []\n    with open(label_path, \"r\") as f:\n        lines = f.readlines()\n\n    annotations = []\n    for line in lines:\n        parts = line.strip().split()\n        if len(parts) >= 5:\n            class_id = int(parts[0])\n            if len(parts) == 5:\n                x_center, y_center, width, height = map(float, parts[1:5])\n                annotations.append([class_id, x_center, y_center, width, height])\n            else:  \n                polygon = [float(x) for x in parts[1:]]\n                x_coords = polygon[0::2]\n                y_coords = polygon[1::2]\n                x_min, x_max = min(x_coords), max(x_coords)\n                y_min, y_max = min(y_coords), max(y_coords)\n                x_center = (x_min + x_max) / 2\n                y_center = (y_min + y_max) / 2\n                width = x_max - x_min\n                height = y_max - y_min\n                annotations.append([class_id, x_center, y_center, width, height])\n    return annotations\n\n\ndef filter_label_0_data(base_path, split=\"train\"):\n    base_path = Path(base_path)\n    images_dir = base_path / split / \"images\"\n    labels_dir = base_path / split / \"labels\"\n\n    image_files = list(images_dir.glob(\"*.jpg\")) + list(images_dir.glob(\"*.png\"))\n    filtered_data = []\n\n    for image_path in image_files:\n        annotations = load_label_file(labels_dir / f\"{image_path.stem}.txt\")\n        class_0_annotations = [ann for ann in annotations if ann[0] == 0]\n        if class_0_annotations:\n            filtered_data.append({\"image_path\": image_path, \"annotations\": class_0_annotations})\n\n    print(f\"[{split}] {len(filtered_data)} images with label 0\")\n    return filtered_data\n\n\ndef visualize_samples(filtered_data, num_samples=6):\n    if not filtered_data:\n        print(\"No data to visualize.\")\n        return\n\n    num_samples = min(num_samples, len(filtered_data))\n    cols, rows = 3, (num_samples + 2) // 3\n    plt.figure(figsize=(15, 5 * rows))\n\n    for i in range(num_samples):\n        data_item = filtered_data[i]\n        image = cv2.imread(str(data_item[\"image_path\"]))\n        if image is None:\n            continue\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        h, w = image.shape[:2]\n\n        for class_id, x_c, y_c, bw, bh in data_item[\"annotations\"]:\n            x1 = int((x_c - bw / 2) * w)\n            y1 = int((y_c - bh / 2) * h)\n            x2 = int((x_c + bw / 2) * w)\n            y2 = int((y_c + bh / 2) * h)\n            cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n\n        plt.subplot(rows, cols, i + 1)\n        plt.imshow(image)\n        plt.title(data_item[\"image_path\"].name)\n        plt.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\n\n# def analyze_dataset_format(base_path, split=\"train\"):\n#     base_path = Path(base_path)\n#     labels_dir = base_path / split / \"labels\"\n#     label_files = list(labels_dir.glob(\"*.txt\"))\n\n#     bbox, seg, files_bbox, files_seg, files_mixed = 0, 0, 0, 0, 0\n#     for lf in label_files:\n#         anns = load_label_file(lf)\n#         bbox_count = sum(1 for ann in anns if len(ann) == 5)\n#         seg_count = len(anns) - bbox_count\n#         bbox += bbox_count\n#         seg += seg_count\n#         if bbox_count > 0 and seg_count > 0:\n#             files_mixed += 1\n#         elif bbox_count > 0:\n#             files_bbox += 1\n#         elif seg_count > 0:\n#             files_seg += 1\n\n#     print(f\"\\n=== {split} set ===\")\n#     print(f\"Files: {len(label_files)} | BBox: {bbox} | Seg: {seg}\")\n#     print(f\"Files (bbox only): {files_bbox}, (seg only): {files_seg}, mixed: {files_mixed}\")\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-03T04:46:22.205528Z","iopub.execute_input":"2025-09-03T04:46:22.205784Z","iopub.status.idle":"2025-09-03T04:46:22.217495Z","shell.execute_reply.started":"2025-09-03T04:46:22.205769Z","shell.execute_reply":"2025-09-03T04:46:22.216910Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Train and Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"\n\nDATASET_CONFIG = {\n    'path': '/kaggle/input/id-bound',\n    'train': 'train/images',\n    'val': 'valid/images',\n    'test': 'test/images',\n    'nc': 1,\n    'names': ['ID']\n}\n\nHYPERPARAMS = [\n    {'lr0': 0.002, 'weight_decay': 0.0005, 'imgsz': 640, 'batch': 16, 'epochs': 35,\n     'degrees': 2, 'translate': 0.05, 'scale': 0.1, 'mosaic': 0.5, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4},\n    {'lr0': 0.001, 'weight_decay': 0.0005, 'imgsz': 640, 'batch': 16, 'epochs': 35,\n     'degrees': 2, 'translate': 0.05, 'scale': 0.1, 'mosaic': 0.5, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4},\n    {'lr0': 0.002, 'weight_decay': 0.0005, 'imgsz': 800, 'batch': 8, 'epochs': 35,\n     'degrees': 2, 'translate': 0.05, 'scale': 0.1, 'mosaic': 0.5, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4},\n    {'lr0': 0.002, 'weight_decay': 0.0005, 'imgsz': 640, 'batch': 16, 'epochs': 35,\n     'degrees': 5, 'translate': 0.1, 'scale': 0.2, 'mosaic': 0.8, 'hsv_h': 0.03, 'hsv_s': 0.5, 'hsv_v': 0.3},\n    {'lr0': 0.002, 'weight_decay': 0.0005, 'imgsz': 640, 'batch': 16, 'epochs': 35,\n     'degrees': 0, 'translate': 0.02, 'scale': 0.05, 'mosaic': 0.2, 'hsv_h': 0.01, 'hsv_s': 0.3, 'hsv_v': 0.2}\n]\n\nCHECKPOINT_FILE = '/kaggle/working/tuning_checkpoint.pkl'\nDATASET_YAML = 'dataset.yaml'\nPROJECT_NAME = 'hyperparam_tuning'\n\n\ndef save_dataset_yaml(config: dict, filename: str = DATASET_YAML):\n    with open(filename, 'w') as f:\n        yaml.dump(config, f)\n    print(f\"Dataset configuration saved to {filename}\")\n\ndef load_checkpoint(filepath: str):\n    if os.path.exists(filepath):\n        try:\n            with open(filepath, 'rb') as f:\n                data = pickle.load(f)\n            print(f\"Resuming from run {data['last_completed_run'] + 1}\")\n            return data\n        except:\n            print(\"Checkpoint exists but not readable, starting fresh...\")\n    else:\n        print(\"Starting fresh hyperparameter tuning...\")\n    return {'last_completed_run': -1, 'results_log': []}\n\ndef save_checkpoint(filepath: str, last_run: int, results_log: list, best_map: float, best_params: dict, best_model_path: str):\n    try:\n        data = {\n            'last_completed_run': last_run,\n            'results_log': results_log,\n            'best_map': best_map,\n            'best_params': best_params,\n            'best_model_path': best_model_path\n        }\n        with open(filepath, 'wb') as f:\n            pickle.dump(data, f)\n        print(f\"Checkpoint saved (run {last_run + 1})\")\n    except:\n        print(\"Checkpoint save failed (continuing anyway)\")\n\ndef train_single_run(run_idx: int, params: dict):\n    print(f\"\\n=== Configuration {run_idx+1}/{len(HYPERPARAMS)} ===\")\n    print(f\"lr0: {params['lr0']}, weight_decay: {params['weight_decay']}, imgsz: {params['imgsz']}, batch: {params['batch']}\")\n    print(f\"degrees: {params['degrees']}, translate: {params['translate']}, scale: {params['scale']}\")\n    print(f\"mosaic: {params['mosaic']}, hsv_h: {params['hsv_h']}, hsv_s: {params['hsv_s']}\")\n\n    model = YOLO('yolov8n.pt')\n    results = model.train(\n        data=DATASET_YAML,\n        epochs=params['epochs'],\n        imgsz=params['imgsz'],\n        batch=params['batch'],\n        lr0=params['lr0'],\n        momentum=0.937,\n        weight_decay=params['weight_decay'],\n        device=0,\n        project=PROJECT_NAME,\n        name=f'run_{run_idx+1}',\n        save=True,\n        plots=True,\n        verbose=False,\n        patience=15,\n        degrees=params['degrees'],\n        translate=params['translate'],\n        scale=params['scale'],\n        shear=2,\n        perspective=0.0,\n        flipud=0.0,\n        fliplr=0.0,\n        mosaic=params['mosaic'],\n        mixup=0.0,\n        copy_paste=0.0,\n        hsv_h=params['hsv_h'],\n        hsv_s=params['hsv_s'],\n        hsv_v=params['hsv_v']\n    )\n    metrics = model.val(data=DATASET_YAML)\n    return metrics.box.map50, metrics.box.map\n\ndef display_results(results_log, best_map, best_params):\n    print(\"\\n=== FINAL HYPERPARAMETER TUNING RESULTS ===\")\n    print(\"Run | lr0   | w_decay | imgsz | batch | degrees | mosaic | mAP@0.5\")\n    print(\"----|-------|---------|-------|-------|---------|--------|--------\")\n    for r in results_log:\n        print(f\"{r['run']:2d}  | {r['lr0']:.3f} | {r['weight_decay']:.4f}  | {r['imgsz']:5d} | {r['batch']:5d} | {r['degrees']:7.1f} | {r['mosaic']:6.1f} | {r['mAP@0.5']:.4f}\")\n\n    print(f\"\\n✓ BEST CONFIGURATION:\")\n    for key, value in best_params.items():\n        print(f\"{key}: {value}\")\n    print(f\"✓ Best mAP@0.5: {best_map:.4f}\")\n\ndef visualize_results(results_log, best_map):\n    plt.figure(figsize=(12, 8))\n    runs = [r['run'] for r in results_log]\n    maps = [r['mAP@0.5'] for r in results_log]\n    colors = ['red' if m == best_map else 'blue' for m in maps]\n\n    plt.bar(runs, maps, color=colors, alpha=0.7)\n    plt.xlabel('Run Number')\n    plt.ylabel('mAP@0.5')\n    plt.title(f'Hyperparameter Tuning Results - Best mAP@0.5: {best_map:.4f}')\n    plt.grid(True, alpha=0.3)\n\n    best_run = next(r['run'] for r in results_log if r['mAP@0.5'] == best_map)\n    plt.annotate(f'Best: {best_map:.4f}',\n                 xy=(best_run, best_map),\n                 xytext=(best_run, best_map + 0.02),\n                 arrowprops=dict(arrowstyle='->', color='red'),\n                 fontsize=12, ha='center')\n\n    plt.tight_layout()\n    plt.savefig('final_hyperparameter_results.png', dpi=150, bbox_inches='tight')\n    plt.show()\n\nsave_dataset_yaml(DATASET_CONFIG)\n\ncheckpoint = load_checkpoint(CHECKPOINT_FILE)\nstart_run = checkpoint['last_completed_run'] + 1\nresults_log = checkpoint['results_log']\nbest_map = max([r['mAP@0.5'] for r in results_log], default=0)\nbest_params = None\nbest_model_path = None\n\nif results_log:\n    best_result = max(results_log, key=lambda x: x['mAP@0.5'])\n    best_params = {k: v for k, v in best_result.items() if k not in ['run', 'mAP@0.5', 'mAP@0.5:0.95', 'model_path']}\n    best_model_path = best_result['model_path']\n\nfor i in range(start_run, len(HYPERPARAMS)):\n    params = HYPERPARAMS[i]\n    try:\n        current_map, current_map_avg = train_single_run(i, params)\n\n        result_entry = {\n            'run': i+1,\n            **params,\n            'mAP@0.5': current_map,\n            'mAP@0.5:0.95': current_map_avg,\n            'model_path': f'{PROJECT_NAME}/run_{i+1}/weights/best.pt'\n        }\n        results_log.append(result_entry)\n\n        print(f\"mAP@0.5: {current_map:.4f}, mAP@0.5:0.95: {current_map_avg:.4f}\")\n\n        if current_map > best_map:\n            best_map = current_map\n            best_params = params.copy()\n            best_model_path = result_entry['model_path']\n            print(f\"✓ New best mAP@0.5: {best_map:.4f}\")\n\n        save_checkpoint(CHECKPOINT_FILE, i, results_log, best_map, best_params, best_model_path)\n\n    except Exception as e:\n        print(f\"Error in run {i+1}: {e}\")\n        print(\"Continuing to next configuration...\")\n        continue\n\ndisplay_results(results_log, best_map, best_params)\nvisualize_results(results_log, best_map)\n\ntry:\n    shutil.make_archive('/kaggle/working/hyperparameter_results', 'zip', PROJECT_NAME)\n    print(\"\\n✓ Results zipped as 'hyperparameter_results.zip' - DOWNLOAD THIS!\")\nexcept:\n    print(\"Zip creation failed, download individual files manually\")\n\nif best_model_path and os.path.exists(best_model_path):\n    best_model = YOLO(best_model_path)\n    best_model.save('BEST_id_detection_model.pt')\n    print(f\"\\n✓ Best model saved as: BEST_id_detection_model.pt\")\n\n    try:\n        if os.path.exists(CHECKPOINT_FILE):\n            os.remove(CHECKPOINT_FILE)\n            print(\"✓ Checkpoint file cleaned up\")\n    except:\n        pass\n\nprint(f\"✓ Final best mAP@0.5: {best_map:.4f}\")\nprint(f\"✓ Results visualization saved to: final_hyperparameter_results.png\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T04:46:22.218840Z","iopub.execute_input":"2025-09-03T04:46:22.219331Z"}},"outputs":[{"name":"stdout","text":"Dataset configuration saved to dataset.yaml\nStarting fresh hyperparameter tuning...\n\n=== Configuration 1/5 ===\nlr0: 0.002, weight_decay: 0.0005, imgsz: 640, batch: 16\ndegrees: 2, translate: 0.05, scale: 0.1\nmosaic: 0.5, hsv_h: 0.015, hsv_s: 0.7\n\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2/6.2MB 80.2MB/s 0.1s\nUltralytics 8.3.191 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset.yaml, degrees=2, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=35, erasing=0.4, exist_ok=False, fliplr=0.0, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.002, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=0.5, multi_scale=False, name=run_1, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=15, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=hyperparam_tuning, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=hyperparam_tuning/run_1, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.1, seed=0, shear=2, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.05, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ━━━━━━━━━━━━ 755.1/755.1KB 16.8MB/s 0.0s\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \nModel summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ━━━━━━━━━━━━ 5.4/5.4MB 66.3MB/s 0.1s\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.4±0.4 ms, read: 9.5±4.6 MB/s, size: 54.0 KB)\n\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/id-bound/train/labels... 4095 images, 73 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 4095/4095 195.0it/s 21.0s<0.0s\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/input/id-bound/train/images/WhatsApp-Image-2023-11-14-at-2-57-11-PM_jpeg_jpg.rf.7cc4f6982b1c95644299c765e198eda7.jpg: 1 duplicate labels removed\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/input/id-bound/train/images/WhatsApp-Image-2023-11-14-at-2-57-11-PM_jpeg_jpg.rf.8616fd1b3cb708c9880b0b715ea8b5dc.jpg: 1 duplicate labels removed\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/input/id-bound/train/images/WhatsApp-Image-2023-11-14-at-2-57-11-PM_jpeg_jpg.rf.d4a7cd5bc8b592d2d149f10e79279a6e.jpg: 1 duplicate labels removed\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/input/id-bound/train/images/contentids-list-10002424-backImage_jpeg_jpg.rf.7f86f90e11b040f723512fea4ff60f92.jpg: 1 duplicate labels removed\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/input/id-bound/train/images/contentids-list-10002424-backImage_jpeg_jpg.rf.f752eed657a09ddbb4c879e485c62be4.jpg: 1 duplicate labels removed\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/input/id-bound/train/images/image_1026_jpg.rf.79cc89121d04d487884d39cd51f4881b.jpg: 1 duplicate labels removed\nWARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/id-bound/train is not writeable, cache not saved.\nWARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 4179, len(boxes) = 4183. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 7.2±2.5 MB/s, size: 41.1 KB)\n\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/id-bound/valid/labels... 1016 images, 21 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1016/1016 125.7it/s 8.1s1s\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/input/id-bound/valid/images/contentids-list-10002424-backImage_jpeg_jpg.rf.4de65d2279e77d38e4079c37a2231615.jpg: 1 duplicate labels removed\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/input/id-bound/valid/images/contentids-list-100049134913-backImage_jpeg_jpg.rf.b0d504a861916fd49a8950ce4d08089a.jpg: 1 duplicate labels removed\nWARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/id-bound/valid is not writeable, cache not saved.\nPlotting labels to hyperparam_tuning/run_1/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.002' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mhyperparam_tuning/run_1\u001b[0m\nStarting training for 35 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K       1/35      2.08G     0.6902      1.325      1.243         24        640: 100% ━━━━━━━━━━━━ 256/256 6.0it/s 42.7s<0.2s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 4.8it/s 6.7s0.2s\n                   all       1016       1039      0.882      0.831      0.896      0.698\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K       2/35      2.56G     0.6898     0.7894      1.224         28        640: 100% ━━━━━━━━━━━━ 256/256 6.6it/s 38.8s<0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 5.8it/s 5.5s0.2s\n                   all       1016       1039      0.895      0.827      0.897      0.704\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K       3/35      2.58G     0.6959     0.6752      1.227         23        640: 100% ━━━━━━━━━━━━ 256/256 6.5it/s 39.1s<0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 5.7it/s 5.6s0.2s\n                   all       1016       1039      0.891       0.87      0.919      0.735\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K       4/35       2.6G     0.6705     0.6182      1.212         24        640: 100% ━━━━━━━━━━━━ 256/256 6.6it/s 39.0s<0.2s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 5.9it/s 5.4s0.2s\n                   all       1016       1039      0.915      0.883      0.931      0.795\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K       5/35      2.61G     0.6231      0.553      1.175         23        640: 100% ━━━━━━━━━━━━ 256/256 6.5it/s 39.1s<0.2s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 5.7it/s 5.6s0.2s\n                   all       1016       1039      0.907      0.903      0.936        0.8\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K       6/35      2.63G     0.5869     0.5183      1.138         22        640: 100% ━━━━━━━━━━━━ 256/256 6.6it/s 39.0s<0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 5.6it/s 5.7s0.2s\n                   all       1016       1039      0.918      0.894      0.946      0.817\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K       7/35      2.65G     0.5702      0.504      1.134         19        640: 100% ━━━━━━━━━━━━ 256/256 6.5it/s 39.3s<0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 5.6it/s 5.7s0.2s\n                   all       1016       1039      0.933       0.92      0.951      0.828\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K       8/35      2.67G     0.5287     0.4561      1.101         21        640: 100% ━━━━━━━━━━━━ 256/256 6.5it/s 39.2s<0.2s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 5.7it/s 5.6s0.2s\n                   all       1016       1039      0.934      0.921      0.958      0.818\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K       9/35      2.68G      0.518     0.4275      1.086         22        640: 100% ━━━━━━━━━━━━ 256/256 6.5it/s 39.1s<0.2s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 5.6it/s 5.7s0.2s\n                   all       1016       1039      0.966      0.936       0.97      0.847\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      10/35       2.7G     0.5083     0.4333      1.078         24        640: 100% ━━━━━━━━━━━━ 256/256 6.5it/s 39.2s<0.2s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 5.8it/s 5.5s0.2s\n                   all       1016       1039      0.966      0.938      0.969      0.843\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      11/35      2.72G     0.4905     0.4033      1.059         26        640: 100% ━━━━━━━━━━━━ 256/256 6.5it/s 39.4s<0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 5.8it/s 5.5s0.2s\n                   all       1016       1039      0.962      0.947      0.972      0.883\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      12/35      2.73G     0.4872     0.3966      1.066         24        640: 100% ━━━━━━━━━━━━ 256/256 6.5it/s 39.3s<0.2s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 5.9it/s 5.4s0.2s\n                   all       1016       1039      0.965      0.952      0.974       0.87\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      13/35      2.75G     0.4687     0.3853       1.05         31        640: 100% ━━━━━━━━━━━━ 256/256 6.5it/s 39.3s<0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 5.9it/s 5.4s0.2s\n                   all       1016       1039      0.968      0.942       0.97      0.856\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      14/35      2.77G     0.4714     0.3775      1.046         26        640: 100% ━━━━━━━━━━━━ 256/256 6.5it/s 39.1s<0.2s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 38% ━━━━──────── 12/32 5.8it/s 2.1s<3.5s","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\nimport os\n\nbest_model_path = '/kaggle/input/id-bound-model/id_detection_best.pt' \n\nif os.path.exists(best_model_path):\n    model = YOLO(best_model_path)\n    print(f\"✓ Loaded best model from {best_model_path}\")\nelse:\n    raise FileNotFoundError(f\"Model not found at {best_model_path}\")\n\nmetrics = model.val(data='/kaggle/input/id-bound-model/dataset_id_detection.yaml', save_json=True, plots=True)\n\nprint(\"\\n=== DETECTION METRICS ===\")\nprint(f\"mAP@0.5: {metrics.box.map50:.4f}\")\nprint(f\"mAP@0.5:0.95: {metrics.box.map:.4f}\")\nprint(f\"Precision: {metrics.box.mp:.4f}\")\nprint(f\"Recall: {metrics.box.mr:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_images = '/kaggle/input/id-bound/test/images'\n\nmetrics = model.val(\n    data='/kaggle/input/id-bound-model/dataset_id_detection.yaml', \n    split='test',   \n    save_json=True,\n    plots=True\n)\n\nprint(\"\\n=== TEST METRICS ===\")\nprint(f\"mAP@0.5: {metrics.box.map50:.4f}\")\nprint(f\"mAP@0.5:0.95: {metrics.box.map:.4f}\")\nprint(f\"Precision: {metrics.box.mp:.4f}\")\nprint(f\"Recall: {metrics.box.mr:.4f}\")\n\n\nresults = model.predict(\n    source=test_images,   \n    save=False,            \n    save_txt=False,       \n    imgsz=640,           \n    conf=0.25,\n    verbose=False)\n\n\nfor r in results[:5]: \n    r.show()     ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Rotation Correction ","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport time\nimport json\nimport itertools\nimport numpy as np\nfrom collections import defaultdict\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\nclass EarlyStopping:\n    def __init__(self, patience=7, min_delta=0.1, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.best_loss = None\n        self.counter = 0\n        self.best_weights = None\n        \n    def __call__(self, val_loss, model):\n        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            self.best_weights = model.state_dict().copy()\n        else:\n            self.counter += 1\n            \n        if self.counter >= self.patience and self.restore_best_weights:\n            model.load_state_dict(self.best_weights)\n            return True\n        return False\n\n# class IDRotationDataset(Dataset):\n#     def __init__(self, image_dir, transform=None, angles=None, rotation_jitter=0, original_angle=0):\n#         self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.lower().endswith(('png','jpg','jpeg'))]\n#         self.transform = transform\n#         self.angles = angles\n#         self.rotation_jitter = rotation_jitter\n#         self.original_angle = original_angle\n    \n#     def __len__(self):\n#         return len(self.image_paths)\n    \n#     def __getitem__(self, idx):\n#         img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n#         target_angle = np.random.choice(self.angles) if self.angles else np.random.uniform(0, 360)\n#         actual_rotation = target_angle + np.random.uniform(-self.rotation_jitter, self.rotation_jitter) if self.rotation_jitter else target_angle\n#         rotation_amount = actual_rotation - self.original_angle\n#         img = np.array(img.rotate(rotation_amount, expand=True, fillcolor=(255,255,255)))\n#         if self.transform:\n#             img = self.transform(image=img)[\"image\"]\n#         target_rad = math.radians(target_angle)\n#         label = torch.tensor([math.sin(target_rad), math.cos(target_rad)], dtype=torch.float)\n#         return img, label\n\ntrain_transform = A.Compose([\n    A.Resize(224, 224),\n    A.Perspective(scale=(0.0, 0.08), p=0.5),\n    A.Affine(scale=(0.8, 1.2), shear=(-10, 10), p=0.5),\n    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n    A.ImageCompression(quality_range=(40, 90), p=0.5),\n    A.Blur(blur_limit=(3, 5), p=0.2),\n    A.GaussNoise(var_limit=(5, 15), p=0.2),\n    A.RandomShadow(p=0.2),\n    A.RandomSunFlare(p=0.2, src_radius=50, flare_roi=(0,0,1,0.5), src_color=(255,255,255)),\n    A.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n    ToTensorV2()\n])\n\nvalid_transform = A.Compose([\n    A.Resize(224, 224),\n    A.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n    ToTensorV2()\n])\n\n# class RotationRegressor(nn.Module):\n#     def __init__(self, backbone_name=\"mobilenet_v3_small\", pretrained=True):\n#         super().__init__()\n#         backbones = {\n#             \"mobilenet_v3_small\": (models.mobilenet_v3_small(pretrained=pretrained), 576),\n#             \"mobilenet_v3_large\": (models.mobilenet_v3_large(pretrained=pretrained), 960),\n#             \"resnet18\": (models.resnet18(pretrained=pretrained), 512),\n#             \"efficientnet_b0\": (models.efficientnet_b0(pretrained=pretrained), 1280),\n#             \"efficientnet_b2\": (models.efficientnet_b2(pretrained=pretrained), 1408)\n#         }\n#         if backbone_name not in backbones:\n#             raise ValueError(f\"Unknown backbone: {backbone_name}\")\n#         backbone, feature_dim = backbones[backbone_name]\n#         if \"mobilenet\" in backbone_name or \"efficientnet\" in backbone_name:\n#             backbone.classifier = nn.Identity()\n#         else:\n#             backbone.fc = nn.Identity()\n#         self.backbone = backbone\n#         self.head = nn.Sequential(nn.Linear(feature_dim, 128), nn.ReLU(), nn.Dropout(0.1), nn.Linear(128,2))\n        \n#     def forward(self, x):\n#         return F.normalize(self.head(self.backbone(x)), dim=1)\n\n# def vector_to_angle(vec):\n#     return math.degrees(math.atan2(vec[0], vec[1])) % 360\n\n# def discretize_angle(angle, classes):\n#     return min(classes, key=lambda x: abs(x - angle))\n\n# def angular_error(preds, labels):\n#     pred_angles = np.array([vector_to_angle(p) for p in preds])\n#     true_angles = np.array([vector_to_angle(l) for l in labels])\n#     diffs = np.abs(pred_angles - true_angles)\n#     diffs = np.minimum(diffs, 360 - diffs)\n#     return np.mean(diffs), np.std(diffs)\n\n# def train_model(config, train_loader, valid_loader, device, max_epochs=30):\n#     model = RotationRegressor(config['backbone']).to(device)\n#     criterion = nn.MSELoss() if config['criterion']=='MSE' else nn.SmoothL1Loss()\n#     optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay']) if config['optimizer']=='Adam' else torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n#     early_stopping = EarlyStopping(patience=7, min_delta=0.1)\n#     train_losses, val_losses, angular_errors = [], [], []\n#     angle_classes = config['angle_classes']\n#     start_time = time.time()\n    \n#     for epoch in range(max_epochs):\n#         model.train()\n#         train_loss = sum(criterion(model(imgs.to(device)), labels.to(device)).item() * imgs.size(0) for imgs, labels in train_loader)/len(train_loader.dataset)\n#         train_losses.append(train_loss)\n#         model.eval()\n#         val_loss, all_pred_vecs, all_true_vecs, all_pred, all_true = 0, [], [], [], []\n#         with torch.no_grad():\n#             for imgs, labels in valid_loader:\n#                 imgs, labels = imgs.to(device), labels.to(device)\n#                 preds = model(imgs)\n#                 val_loss += criterion(preds, labels).item()*imgs.size(0)\n#                 all_pred_vecs.extend(preds.cpu().numpy())\n#                 all_true_vecs.extend(labels.cpu().numpy())\n#                 if config['evaluation_mode']=='classification':\n#                     for p,l in zip(preds,labels):\n#                         all_pred.append(discretize_angle(vector_to_angle(p.cpu().numpy()), angle_classes))\n#                         all_true.append(discretize_angle(vector_to_angle(l.cpu().numpy()), angle_classes))\n#         val_loss /= len(valid_loader.dataset)\n#         val_losses.append(val_loss)\n#         mean_ang_err, std_ang_err = angular_error(all_pred_vecs, all_true_vecs)\n#         angular_errors.append(mean_ang_err)\n#         if config['evaluation_mode']=='classification':\n#             acc = accuracy_score(all_true, all_pred)\n#             prec, rec, f1, _ = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)\n#             print(f\"Epoch {epoch+1}/{max_epochs} | TL: {train_loss:.4f} | VL: {val_loss:.4f} | Acc: {acc:.3f} | F1: {f1:.3f} | AngErr: {mean_ang_err:.2f}°±{std_ang_err:.2f}\")\n#         else:\n#             print(f\"Epoch {epoch+1}/{max_epochs} | TL: {train_loss:.4f} | VL: {val_loss:.4f} | AngErr: {mean_ang_err:.2f}°±{std_ang_err:.2f}\")\n#         if early_stopping(mean_ang_err, model):\n#             print(f\"Early stopping at epoch {epoch+1}\")\n#             break\n\n#     training_time = time.time() - start_time\n#     return {\n#         'config': config,\n#         'best_angular_error': min(angular_errors),\n#         'final_angular_error': angular_errors[-1],\n#         'training_time': training_time,\n#         'epochs_trained': len(train_losses),\n#         'train_losses': train_losses,\n#         'val_losses': val_losses,\n#         'angular_errors': angular_errors\n#     }\n\n# def get_hyperparameter_grid():\n#     criterions = ['MSE','SmoothL1Loss']\n#     optimizers = ['Adam','AdamW']\n#     learning_rates = [1e-3,5e-4,1e-4,1e-5]\n#     weight_decays = [0,1e-4]\n#     backbones = ['mobilenet_v3_small','mobilenet_v3_large','resnet18','efficientnet_b0','efficientnet_b2']\n#     angle_configs = [\n#         {'name':'8_classes_coarse','angles':[0,45,90,135,180,225,270,315],'angle_classes':[0,45,90,135,180,225,270,315],'evaluation_mode':'classification'},\n#         {'name':'16_classes_fine','angles':[i*22.5 for i in range(16)],'angle_classes':[i*22.5 for i in range(16)],'evaluation_mode':'classification'},\n#         {'name':'pure_regression','angles':None,'angle_classes':None,'evaluation_mode':'regression'}\n#     ]\n#     rotation_jitters = [0,5]\n#     batch_sizes = [16,32,64]\n#     return [\n#         {\n#             'criterion':c,'optimizer':o,'lr':lr,'weight_decay':wd,'backbone':b,\n#             'angles':ac['angles'],'angle_classes':ac['angle_classes'],'evaluation_mode':ac['evaluation_mode'],\n#             'angle_config_name':ac['name'],'rotation_jitter':j,'batch_size':bs\n#         }\n#         for c,o,lr,wd,b,ac,j,bs in itertools.product(criterions,optimizers,learning_rates,weight_decays,backbones,angle_configs,rotation_jitters,batch_sizes)\n#     ]\n\n# def run_hyperparameter_search(image_dir_train, image_dir_valid, max_configs=50):\n#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n#     param_grid = get_hyperparameter_grid()\n#     if len(param_grid) > max_configs:\n#         param_grid = np.random.choice(param_grid, max_configs, replace=False).tolist()\n#     all_results = []\n#     for config in param_grid:\n#         try:\n#             train_loader = DataLoader(IDRotationDataset(image_dir_train, transform=train_transform, angles=config['angles'], rotation_jitter=config['rotation_jitter']), batch_size=config['batch_size'], shuffle=True, num_workers=2)\n#             valid_loader = DataLoader(IDRotationDataset(image_dir_valid, transform=valid_transform, angles=config['angles'], rotation_jitter=0), batch_size=config['batch_size'], shuffle=False, num_workers=2)\n#             all_results.append(train_model(config, train_loader, valid_loader, device))\n#             torch.cuda.empty_cache()\n#         except Exception as e:\n#             print(f\"Error: {e}\")\n#     return all_results\n\n# def analyze_results(results):\n#     if not results: return None\n#     results.sort(key=lambda x: x['best_angular_error'])\n#     fig, axes = plt.subplots(2,3,figsize=(18,12))\n#     backbones = [r['config']['backbone'] for r in results]\n#     angular_errors = [r['best_angular_error'] for r in results]\n#     def plot_bar(ax, keys, values, title): ax.bar(range(len(keys)), values); ax.set_xticks(range(len(keys))); ax.set_xticklabels(keys, rotation=45); ax.set_title(title)\n#     backbone_means = [np.mean([ae for b,ae in zip(backbones,angular_errors) if b==bk]) for bk in set(backbones)]\n#     plot_bar(axes[0,0], list(set(backbones)), backbone_means, \"Angular Error by Backbone\")\n#     best_result = results[0]\n#     axes[1,2].plot(best_result['angular_errors'], 'b-'); axes[1,2].set_title(f'Best Model Training Curve\\nError: {best_result[\"best_angular_error\"]:.2f}°')\n#     plt.tight_layout(); plt.show()\n#     return best_result\n\n# TRAIN_DIR = \"/kaggle/input/idtextlines/train/images\"\n# VALID_DIR = \"/kaggle/input/idtextlines/valid/images\"\n# # results = run_hyperparameter_search(TRAIN_DIR, VALID_DIR, max_configs=10)\n# # best_config = analyze_results(results)\n# # with open('hyperparameter_search_results.json','w') as f:\n# #     for r in results: \n# #         for k in ['train_losses','val_losses','angular_errors']: r[k] = [float(x) for x in r[k]]\n# #     json.dump(results,f,indent=2)\n# # print(f\"Best angular error: {best_config['best_angular_error']:.2f}°\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n\n\nclass IDRotationDataset(Dataset):\n    \n    def __init__(self, image_dir, transform=None, angles=None, rotation_jitter=0, original_angle=0):\n        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir)\n                            if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n        self.transform = transform\n        self.angles = angles\n        self.rotation_jitter = rotation_jitter\n        self.original_angle = original_angle\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n\n        target_angle = np.random.choice(self.angles) if self.angles else np.random.uniform(0, 360)\n        jitter = np.random.uniform(-self.rotation_jitter, self.rotation_jitter) if self.rotation_jitter else 0\n        rotation_amount = target_angle + jitter - self.original_angle\n\n        img = img.rotate(rotation_amount, expand=True, fillcolor=(255, 255, 255))\n        img = np.array(img)\n        if self.transform:\n            img = self.transform(image=img)[\"image\"]\n\n        angle_rad = math.radians(target_angle)\n        label = torch.tensor([math.sin(angle_rad), math.cos(angle_rad)], dtype=torch.float)\n\n        return img, label\n\n=\ndef get_transforms():\n    train_transform = A.Compose([\n        A.Resize(224, 224),\n        A.Perspective(scale=(0.0, 0.08), p=0.5),\n        A.Affine(scale=(0.8, 1.2), shear=(-10, 10), p=0.5),\n        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n        A.ImageCompression(quality_range=(40, 90), p=0.5),\n        A.Blur(blur_limit=(3, 5), p=0.2),\n        A.GaussNoise(var_limit=(5, 15), p=0.2),\n        A.RandomShadow(p=0.2),\n        A.RandomSunFlare(p=0.2, src_radius=50, flare_roi=(0,0,1,0.5), src_color=(255,255,255)),\n        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n        ToTensorV2()\n    ])\n\n    valid_transform = A.Compose([\n        A.Resize(224, 224),\n        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n        ToTensorV2()\n    ])\n    \n    return train_transform, valid_transform\n\ndef create_dataloaders(train_dir, valid_dir, angles, batch_size=16, rotation_jitter=5, num_workers=2):\n    train_transform, valid_transform = get_transforms()\n    \n    train_dataset = IDRotationDataset(train_dir, transform=train_transform, angles=angles, rotation_jitter=rotation_jitter)\n    valid_dataset = IDRotationDataset(valid_dir, transform=valid_transform, angles=angles, rotation_jitter=0)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    \n    return train_loader, valid_loader\n\nclass RotationRegressor(nn.Module):\n    \n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.backbone = models.mobilenet_v3_small(pretrained=pretrained)\n        self.backbone.classifier = nn.Identity()\n        self.head = nn.Sequential(\n            nn.Linear(576, 128),\n            nn.ReLU(),\n            nn.Linear(128, 2)\n        )\n    \n    def forward(self, x):\n        features = self.backbone(x)\n        out = self.head(features)\n        return F.normalize(out, dim=1)  \n\n\nangle_classes = [0, 45, 90, 135, 180, 225, 270, 315]\n\ndef vector_to_angle(vec):\n    angle = math.degrees(math.atan2(vec[0], vec[1]))\n    return angle % 360\n\ndef discretize_angle(angle, classes=angle_classes):\n    return min(classes, key=lambda x: abs(x - angle))\n\ndef angular_error(preds, labels):\n    pred_angles = np.array([vector_to_angle(p) for p in preds])\n    true_angles = np.array([vector_to_angle(l) for l in labels])\n    diffs = np.abs(pred_angles - true_angles)\n    diffs = np.minimum(diffs, 360 - diffs)\n    return np.mean(diffs), np.std(diffs)\n\ndef train_model(model, train_loader, valid_loader, criterion, optimizer, device, num_epochs=30, early_stopping=None):\n    train_losses, val_losses = [], []\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for imgs, labels in train_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            preds = model(imgs)\n            loss = criterion(preds, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * imgs.size(0)\n        train_loss = running_loss / len(train_loader.dataset)\n        train_losses.append(train_loss)\n\n        model.eval()\n        val_loss, all_true, all_pred, all_true_vecs, all_pred_vecs = 0.0, [], [], [], []\n        with torch.no_grad():\n            for imgs, labels in valid_loader:\n                imgs, labels = imgs.to(device), labels.to(device)\n                preds = model(imgs)\n                loss = criterion(preds, labels)\n                val_loss += loss.item() * imgs.size(0)\n\n                all_pred_vecs.extend(preds.cpu().numpy())\n                all_true_vecs.extend(labels.cpu().numpy())\n                for p, l in zip(preds, labels):\n                    all_pred.append(discretize_angle(vector_to_angle(p.cpu().numpy())))\n                    all_true.append(discretize_angle(vector_to_angle(l.cpu().numpy())))\n\n        val_loss /= len(valid_loader.dataset)\n        val_losses.append(val_loss)\n        mean_ang_err, std_ang_err = angular_error(all_pred_vecs, all_true_vecs)\n        acc = accuracy_score(all_true, all_pred)\n        prec, rec, f1, _ = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)\n\n        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n              f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n              f\"Acc: {acc:.3f} | Prec: {prec:.3f} | Rec: {rec:.3f} | F1: {f1:.3f} | \"\n              f\"MeanAngErr: {mean_ang_err:.2f}° ± {std_ang_err:.2f}\")\n\n        if early_stopping and early_stopping(val_loss, model):\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n    return train_losses, val_losses, all_true, all_pred\n\n\ndef plot_training_curve(train_losses, val_losses):\n    plt.figure(figsize=(8,5))\n    plt.plot(train_losses, label=\"Train Loss\")\n    plt.plot(val_losses, label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.title(\"Training & Validation Loss\")\n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred, classes=angle_classes):\n    cm = confusion_matrix(y_true, y_pred, labels=classes)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n    disp.plot(cmap=\"Blues\", xticks_rotation=45)\n    plt.title(\"Confusion Matrix (Final Epoch)\")\n    plt.show()\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nangles = [0, 45, 90, 135, 180, 225, 270, 315]\ntrain_loader, valid_loader = create_dataloaders(\n    \"/kaggle/input/idtextlines/train/images\",\n    \"/kaggle/input/idtextlines/valid/images\",\n    angles\n)\n\nmodel = RotationRegressor(pretrained=True).to(device)\ncriterion = nn.SmoothL1Loss()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\nearly_stopping = EarlyStopping(patience=5, min_delta=0.001, restore_best_weights=True)\n\ntrain_losses, val_losses, all_true, all_pred = train_model(model, train_loader, valid_loader, criterion, optimizer, device, 30, early_stopping)\nplot_training_curve(train_losses, val_losses)\nplot_confusion_matrix(all_true, all_pred)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"import torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = RotationRegressor().to(device)\n\ncheckpoint_path = \"/kaggle/input/rotation-correction-model/RotationClassification.pth\"\nmodel.load_state_dict(torch.load(checkpoint_path, map_location=device), strict=False)\n\nmodel.eval()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"valid_dataset = IDRotationDataset(\n    image_dir=\"/kaggle/input/idtextlines/valid/images\",  \n    transform=valid_transform,  \n    angles=None,                \n    rotation_jitter=0,          \n    original_angle=0\n)\n\nvalid_loader = DataLoader(\n    valid_dataset,\n    batch_size=32,     \n    shuffle=False,    \n    num_workers=2     \n)\nmodel.eval()\nall_pred_vecs, all_true_vecs = [], []\n\nwith torch.no_grad():\n    for imgs, labels in valid_loader:  \n        imgs, labels = imgs.to(device), labels.to(device)\n        preds = model(imgs)\n        all_pred_vecs.extend(preds.cpu().numpy())\n        all_true_vecs.extend(labels.cpu().numpy())\n\nmean_ang_err, std_ang_err = angular_error(all_pred_vecs, all_true_vecs)\nprint(f\"Mean Angular Error: {mean_ang_err:.2f}° ± {std_ang_err:.2f}°\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ID Lines Detection","metadata":{}},{"cell_type":"markdown","source":"## Train and Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport random\nfrom datetime import datetime\nimport yaml\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom ultralytics import YOLO\n\ndef create_dataset_config():\n    dataset_config = {\n        'path': '/kaggle/input/idtextlines',\n        'train': 'train/images',\n        'val': 'valid/images',\n        'nc': 1,\n        'names': ['text_line']\n    }\n    with open('textlines_dataset.yaml', 'w') as f:\n        yaml.dump(dataset_config, f)\n    print(\"Dataset config created: textlines_dataset.yaml\")\n\n\ndef sample_params():\n    return {\n        'epochs': random.choice([30, 50, 80, 100]),\n        'batch': random.choice([8, 16, 24, 32]),\n        'lr0': random.uniform(0.0005, 0.02),\n        'lrf': random.uniform(0.01, 0.2),\n        'momentum': random.uniform(0.8, 0.95),\n        'weight_decay': random.uniform(0.0001, 0.001),\n        'warmup_epochs': random.uniform(1, 5),\n        'warmup_momentum': random.uniform(0.5, 0.9),\n        'box': random.uniform(5, 10),\n        'cls': random.uniform(0.3, 1.0),\n        'dfl': random.uniform(1.0, 2.0),\n        'hsv_h': random.uniform(0.01, 0.02),\n        'hsv_s': random.uniform(0.5, 0.9),\n        'hsv_v': random.uniform(0.3, 0.6),\n        'degrees': random.uniform(5, 15),\n        'translate': random.uniform(0.05, 0.2),\n        'scale': random.uniform(0.3, 0.7),\n        'mixup': random.uniform(0.0, 0.3)\n    }\n\n\ndef random_search_tuning(n_trials=20):\n    import random\n    \n    results_log = []\n    best_map = 0\n    best_params = None\n    \n    print(f\"Starting random search with {n_trials} trials...\")\n    \n    for trial in range(n_trials):\n        params = sample_params()\n        print(f\"\\nTrial {trial+1}/{n_trials}\")\n        print(f\"Parameters: {params}\")\n        print(f\"Early stopping patience: 10 epochs\")\n        \n        try:\n            model = YOLO('yolov8n.pt')\n            \n            results = model.train(\n                data='textlines_dataset.yaml',\n                project='random_search',\n                name=f'trial_{trial}',\n                verbose=False,\n                plots=False,\n                **params\n            )\n            \n            map_score = results.metrics['metrics/mAP50(B)']\n            \n            result_entry = {\n                'trial': trial,\n                'map50': map_score,\n                **params,\n                'timestamp': datetime.now().isoformat()\n            }\n            results_log.append(result_entry)\n            \n            if map_score > best_map:\n                best_map = map_score\n                best_params = params\n                print(f\"New best mAP: {map_score:.4f}\")\n            \n        except Exception as e:\n            print(f\"Error in trial {trial}: {str(e)}\")\n    \n    with open('random_search_results.json', 'w') as f:\n        json.dump(results_log, f, indent=2)\n    \n    return best_params, results_log\n\n    study = optuna.create_study(direction='maximize')\n    study.optimize(objective, n_trials=n_trials)\n    \n    best_params = study.best_params\n    best_score = study.best_value\n    \n    print(f\"Best parameters: {best_params}\")\n    print(f\"Best mAP50: {best_score:.4f}\")\n    print(f\"Best early stopping patience: {best_params['patience']} epochs\")\n    \n    df = study.trials_dataframe()\n    df.to_csv('optuna_results.csv', index=False)\n    \n    return best_params, study\n\n\n\ndef analyze_hyperparameter_results():\n    results_files = [\n        'grid_search_results.json',\n        'random_search_results.json',\n        'optuna_results.csv'\n    ]\n    all_results = []\n\n    for file in results_files[:2]:\n        if os.path.exists(file):\n            with open(file, 'r') as f:\n                data = json.load(f)\n                for entry in data:\n                    entry['method'] = file.split('_')[0]\n                    all_results.append(entry)\n\n    if os.path.exists('optuna_results.csv'):\n        df_optuna = pd.read_csv('optuna_results.csv')\n        for _, row in df_optuna.iterrows():\n            if 'value' in row and not pd.isna(row['value']):\n                entry = {\n                    'map50': row['value'],\n                    'method': 'optuna',\n                    'experiment': row['number']\n                }\n                param_cols = [col for col in df_optuna.columns if col.startswith('params_')]\n                for col in param_cols:\n                    entry[col.replace('params_', '')] = row[col]\n                all_results.append(entry)\n\n    if not all_results:\n        print(\"No tuning results found. Run tuning first.\")\n        return\n\n    df = pd.DataFrame(all_results)\n    plt.figure(figsize=(15, 10))\n\n    plt.subplot(2, 3, 1)\n    if 'method' in df.columns:\n        df.boxplot(column='map50', by='method', ax=plt.gca())\n        plt.title('mAP50 by Tuning Method')\n        plt.suptitle('')\n\n    plt.subplot(2, 3, 2)\n    if 'lr0' in df.columns:\n        plt.scatter(df['lr0'], df['map50'], alpha=0.6)\n        plt.xlabel('Learning Rate')\n        plt.ylabel('mAP50')\n        plt.title('Learning Rate vs mAP50')\n\n    plt.subplot(2, 3, 3)\n    if 'batch' in df.columns:\n        df.boxplot(column='map50', by='batch', ax=plt.gca())\n        plt.title('mAP50 by Batch Size')\n        plt.suptitle('')\n\n    plt.subplot(2, 3, 4)\n    if 'epochs' in df.columns:\n        plt.scatter(df['epochs'], df['map50'], alpha=0.6)\n        plt.xlabel('Epochs')\n        plt.ylabel('mAP50')\n        plt.title('Epochs vs mAP50')\n\n    plt.subplot(2, 3, 5)\n    if 'patience' in df.columns:\n        plt.scatter(df['patience'], df['map50'], alpha=0.6)\n        plt.xlabel('Early Stopping Patience')\n        plt.ylabel('mAP50')\n        plt.title('Early Stopping Patience vs mAP50')\n\n    plt.subplot(2, 3, 6)\n    top_10 = df.nlargest(10, 'map50')\n    plt.barh(range(len(top_10)), top_10['map50'])\n    plt.yticks(range(len(top_10)), [f\"Exp {i}\" for i in top_10.index])\n    plt.xlabel('mAP50')\n    plt.title('Top 10 Experiments')\n\n    plt.tight_layout()\n    plt.savefig('hyperparameter_analysis.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n    best_result = df.loc[df['map50'].idxmax()]\n    print(f\"\\nBest overall result:\")\n    print(f\"mAP50: {best_result['map50']:.4f}\")\n    print(f\"Method: {best_result.get('method', 'unknown')}\")\n    print(f\"Early stopping patience: {best_result.get('patience', 'N/A')}\")\n\n    return df\n\n\n# if __name__ == \"__main__\":\n#     create_dataset_config()\n\n#     best_random, random_log = random_search_tuning(n_trials=10)\n\n\n#     best_model_path = os.path.join('random_search', 'best.pt')\n#     if os.path.exists(best_model_path):\n#         model = YOLO(best_model_path)\n#         results = model.train(\n#             data='textlines_dataset.yaml',\n#             epochs=50,\n#             lr0=0.001,\n#             patience=10,\n#             project='fine_tuning',\n#             name='resumed_training'\n#         )\n#         print(\"\\nResumed training completed.\")\n\n#     analyze_hyperparameter_results()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\n\nbest_model_path = '/kaggle/input/id-lines-detection/best.pt'\n\nmodel = YOLO(best_model_path)\n\nresults = model.val(\n    data='textlines_dataset.yaml',\n    batch=32,\n    imgsz=640,\n    verbose=False\n)\nmetrics_dict = results.results_dict\n\nprint(\"Validation Results:\")\nprint(f\"mAP50: {metrics_dict.get('metrics/mAP50(B)', 'N/A'):.4f}\")\nprint(f\"Precision: {metrics_dict.get('metrics/precision(B)', 'N/A'):.4f}\")\nprint(f\"Recall: {metrics_dict.get('metrics/recall(B)', 'N/A'):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}