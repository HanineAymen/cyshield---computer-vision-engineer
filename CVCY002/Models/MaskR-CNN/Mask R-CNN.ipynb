{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7376095,"sourceType":"datasetVersion","datasetId":4283274},{"sourceId":12945212,"sourceType":"datasetVersion","datasetId":8192072},{"sourceId":12946415,"sourceType":"datasetVersion","datasetId":8192821}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Train and Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"import os\nimport ast\nimport gc\nimport json\nimport time\nimport random\nfrom datetime import datetime\nfrom collections import defaultdict, Counter\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n\nBASE_PATH = \"/kaggle/input/deepfashion2-original-with-dataframes/DeepFashion2\"\nIMAGE_PATHS = {\n    \"train\": f\"{BASE_PATH}/deepfashion2_original_images/train/image\",\n    \"validation\": f\"{BASE_PATH}/deepfashion2_original_images/validation/image\",\n    \"test\": f\"{BASE_PATH}/deepfashion2_original_images/test/test/image\",\n}\nCSV_PATHS = {\n    \"train\": f\"{BASE_PATH}/img_info_dataframes/train.csv\",\n    \"validation\": f\"{BASE_PATH}/img_info_dataframes/validation.csv\",\n    \"test\": f\"{BASE_PATH}/img_info_dataframes/test.csv\",\n}\n\nIMAGE_SIZE = (224, 224)\n\nCLASSES = [\n    \"background\",\n    \"short sleeve top\",\n    \"trousers\",\n    \"shorts\",\n    \"long sleeve top\",\n    \"skirt\",\n    \"vest dress\",\n    \"short sleeve dress\",\n    \"vest\",\n    \"long sleeve outwear\",\n    \"long sleeve dress\",\n    \"sling dress\",\n    \"sling\",\n    \"short sleeve outwear\",\n]\nNUM_CLASSES = len(CLASSES)\n\nCLASS_WEIGHTS_DICT = {\n    \"short sleeve top\": 0.335,\n    \"trousers\": 0.434,\n    \"shorts\": 0.656,\n    \"long sleeve top\": 0.666,\n    \"skirt\": 0.779,\n    \"vest dress\": 1.338,\n    \"short sleeve dress\": 1.395,\n    \"vest\": 1.492,\n    \"long sleeve outwear\": 1.785,\n    \"long sleeve dress\": 3.037,\n    \"sling dress\": 3.699,\n    \"sling\": 12.098,\n    \"short sleeve outwear\": 44.225,\n}\nCLASS_WEIGHTS = torch.tensor([1.0] + [CLASS_WEIGHTS_DICT[c] for c in CLASSES[1:]], dtype=torch.float32)\n\ndef set_seed(seed: int = 42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\nset_seed(42)\n\n\ndef safe_read_csv(path: str) -> pd.DataFrame:\n    try:\n        df = pd.read_csv(path)\n        print(f\"âœ… Loaded dataset: {len(df)} samples | columns: {list(df.columns)}\")\n        return df\n    except FileNotFoundError:\n        print(f\"âŒ Could not find CSV file at {path}\")\n        return pd.DataFrame()\n\n\ndef load_dataset(split: str) -> pd.DataFrame:\n    return safe_read_csv(CSV_PATHS[split])\n\n\ndef load_image(filename: str, split: str) -> np.ndarray:\n\n        full_path = os.path.join(IMAGE_PATHS[split], filename)\n    if not os.path.exists(full_path):\n        print(f\"âŒ Image not found: {full_path}\")\n        return None\n    try:\n        img = cv2.imread(full_path)\n        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    except Exception as exc:\n        print(f\"âŒ Error loading image {full_path}: {exc}\")\n        return None\n\n\n\ndef parse_segmentation(seg_text) -> list:\n\n    if seg_text is None:\n        return []\n    if isinstance(seg_text, float) and pd.isna(seg_text):\n        return []\n\n    if isinstance(seg_text, list):\n        return [np.array(poly, dtype=np.int32).reshape(-1, 2) for poly in seg_text if len(poly) >= 6]\n\n    if isinstance(seg_text, np.ndarray):\n        arr = seg_text.astype(np.int32)\n        return [arr.reshape(-1, 2)] if arr.size >= 6 else []\n\n    if isinstance(seg_text, str):\n        s = seg_text.strip()\n        if s in {\"\", \"[]\", \"nan\", \"none\", \"None\"}:\n            return []\n        try:\n            parsed = ast.literal_eval(s)\n            if isinstance(parsed, list):\n                return [np.array(poly, dtype=np.int32).reshape(-1, 2) for poly in parsed if len(poly) >= 6]\n        except Exception:\n            return []\n\n    return []\n\n\ndef create_mask(polygons: list, image_shape) -> np.ndarray:\n    mask = np.zeros(image_shape[:2], dtype=np.uint8)\n    for poly in polygons:\n        poly_arr = np.asarray(poly, dtype=np.int32)\n        if poly_arr.ndim == 2 and poly_arr.shape[0] >= 3 and poly_arr.shape[1] == 2:\n            cv2.fillPoly(mask, [poly_arr], 255)\n    return mask\n\n\ndef clip_and_validate_bbox(bbox, img_w, img_h):\n    x1, y1, x2, y2 = bbox\n    x1 = max(0, min(x1, img_w - 1))\n    x2 = max(0, min(x2, img_w - 1))\n    y1 = max(0, min(y1, img_h - 1))\n    y2 = max(0, min(y2, img_h - 1))\n    return None if (x2 <= x1 or y2 <= y1) else [x1, y1, x2, y2]\n\n\n\ntrain_transform = A.Compose(\n    [\n        A.Resize(*IMAGE_SIZE),\n        A.HorizontalFlip(p=0.5),\n        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ],\n    bbox_params=A.BboxParams(format=\"pascal_voc\", min_visibility=0.1, label_fields=[\"labels\"]),\n)\n\neval_transform = A.Compose(\n    [\n        A.Resize(*IMAGE_SIZE),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ],\n    bbox_params=A.BboxParams(format=\"pascal_voc\", min_visibility=0.0, label_fields=[\"labels\"], clip=True),\n)\n\n\n\nclass DeepFashionMaskRCNNDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, split: str = \"train\"):\n        self.df = df.reset_index(drop=True)\n        self.split = split\n        if split != \"test\" and \"category_name\" in df.columns:\n            cats = sorted(df[\"category_name\"].dropna().unique().tolist())\n            self.category_to_id = {c: i + 1 for i, c in enumerate(cats)}\n        else:\n            self.category_to_id = {}\n\n    def __len__(self):\n        return len(self.df)\n\n    def _parse_bbox(self, bbox_str, img_w, img_h):\n\n        try:\n            bbox = ast.literal_eval(bbox_str) if isinstance(bbox_str, str) else bbox_str\n            if not bbox or len(bbox) != 4:\n                return []\n            x, y, w, h = bbox\n            x1 = max(0, x)\n            y1 = max(0, y)\n            x2 = min(img_w - 1, x + w)\n            y2 = min(img_h - 1, y + h)\n            if x2 <= x1 or y2 <= y1:\n                return []\n            return [[x1, y1, x2, y2]]\n        except Exception as exc:\n            print(f\"Error parsing bbox: {exc}\")\n            return []\n\n    def _bbox_is_valid(self, bbox):\n        x1, y1, x2, y2 = bbox\n        if not all(isinstance(v, (int, float)) for v in bbox):\n            return False\n        if x2 <= x1 + 2.0 or y2 <= y1 + 2.0:\n            return False\n        if x1 < 0 or y1 < 0 or x2 > IMAGE_SIZE[1] or y2 > IMAGE_SIZE[0]:\n            return False\n        return True\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n\n        image = load_image(row[\"path\"], self.split)\n        if image is None:\n            image = np.zeros((IMAGE_SIZE[0], IMAGE_SIZE[1], 3), dtype=np.uint8)\n        img_h, img_w = image.shape[:2]\n\n        boxes = self._parse_bbox(row.get(\"b_box\"), img_w, img_h)\n\n        labels = []\n        if boxes and self.split != \"test\":\n            category_name = row.get(\"category_name\")\n            if category_name and not pd.isna(category_name):\n                cid = self.category_to_id.get(category_name)\n                if cid is not None:\n                    labels = [int(cid)] * len(boxes)\n\n        masks = []\n        if self.split != \"test\":\n            polys = parse_segmentation(row.get(\"segmentation\"))\n            if polys:\n                mask = create_mask(polys, image.shape)\n                if mask is not None and mask.sum() > 0:\n                    masks = [mask.astype(np.uint8)]\n\n        transform = train_transform if self.split == \"train\" else eval_transform\n        try:\n            transformed = transform(image=image, masks=masks, bboxes=boxes, labels=labels)\n        except Exception as exc:\n            print(f\"Transform error at idx {idx} ({row.get('path')}): {exc}\")\n            return self.__getitem__((idx + 1) % len(self.df))\n\n        image_t = transformed[\"image\"]\n        boxes_t = (\n            torch.as_tensor(transformed[\"bboxes\"], dtype=torch.float32)\n            if transformed[\"bboxes\"]\n            else torch.zeros((0, 4), dtype=torch.float32)\n        )\n        labels_t = (\n            torch.as_tensor(transformed[\"labels\"], dtype=torch.int64)\n            if transformed[\"labels\"]\n            else torch.zeros((0,), dtype=torch.int64)\n        )\n\n        if transformed.get(\"masks\"):\n            masks_t = torch.stack([torch.as_tensor(m, dtype=torch.uint8) for m in transformed[\"masks\"]])\n        else:\n            masks_t = torch.zeros((0, IMAGE_SIZE[0], IMAGE_SIZE[1]), dtype=torch.uint8)\n\n        if len(transformed[\"bboxes\"]) == 0 and len(boxes) > 0:\n            print(f\"Dropped all boxes at idx {idx}, path={row.get('path')}\")\n\n        area = (boxes_t[:, 2] - boxes_t[:, 0]) * (boxes_t[:, 3] - boxes_t[:, 1]) if len(boxes_t) > 0 else torch.zeros((0,), dtype=torch.float32)\n\n        target = {\n            \"boxes\": boxes_t,\n            \"labels\": labels_t,\n            \"masks\": masks_t,\n            \"image_id\": torch.tensor([idx]),\n            \"area\": area,\n            \"iscrowd\": torch.zeros((len(boxes_t),), dtype=torch.int64),\n        }\n\n        if self.split == \"test\":\n            return image_t, {}\n        return image_t, target\n\n\ndef collate_fn(batch):\n    batch = [b for b in batch if b is not None]\n    if not batch:\n        dummy_img = torch.zeros((3, IMAGE_SIZE[0], IMAGE_SIZE[1]), dtype=torch.float32)\n        dummy_tgt = {\n            \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n            \"labels\": torch.zeros((0,), dtype=torch.int64),\n            \"masks\": torch.zeros((0, IMAGE_SIZE[0], IMAGE_SIZE[1]), dtype=torch.uint8),\n            \"image_id\": torch.tensor([0]),\n            \"area\": torch.zeros((0,), dtype=torch.float32),\n            \"iscrowd\": torch.zeros((0,), dtype=torch.int64),\n        }\n        return [dummy_img], [dummy_tgt]\n    images, targets = zip(*batch)\n    return list(images), list(targets)\n\n\n\ndef build_weighted_maskrcnn(num_classes: int, class_weights: torch.Tensor = None, device: str = \"cuda\"):\n    weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n    model = maskrcnn_resnet50_fpn(weights=weights)\n\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n\n    if class_weights is not None:\n        w = class_weights.detach().float().to(device)\n\n        def weighted_classifier_loss(class_logits, labels):\n            return F.cross_entropy(class_logits, labels, weight=w)\n\n        model.roi_heads.fastrcnn_loss = lambda class_logits, labels, *args, **kwargs: weighted_classifier_loss(class_logits, labels)\n\n    return model\n\n\ndef train_maskrcnn_epoch(model, loader, optimizer, device):\n    model.train()\n    total_loss = 0.0\n    component_sums = defaultdict(float)\n\n    for i, (images, targets) in enumerate(loader):\n        images = [img.to(device) for img in images]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        optimizer.zero_grad()\n        loss_dict = model(images, targets)\n        loss = sum(loss_dict.values())\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        for k, v in loss_dict.items():\n            component_sums[k] += v.item()\n\n        if i % 1000 == 0:\n            comps = \", \".join([f\"{k}:{v.item():.4f}\" for k, v in loss_dict.items()])\n            print(f\"Batch {i}/{len(loader)} | Loss {loss.item():.4f} | {comps}\")\n\n    avg_components = {k: v / max(1, len(loader)) for k, v in component_sums.items()}\n    return total_loss / max(1, len(loader)), avg_components\n\n\ndef validate_maskrcnn(model, loader, device):\n    model.train()\n    total_loss = 0.0\n    component_sums = defaultdict(float)\n\n    with torch.no_grad():\n        for images, targets in loader:\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            loss_dict = model(images, targets)\n            loss = sum(loss_dict.values())\n            total_loss += loss.item()\n            for k, v in loss_dict.items():\n                component_sums[k] += v.item()\n\n    avg_components = {k: v / max(1, len(loader)) for k, v in component_sums.items()}\n    return total_loss / max(1, len(loader)), avg_components\n\n\ndef train_model(model, train_loader, optimizer, device, num_epochs=5, save_dir=\"training_results\", val_loader=None):\n    os.makedirs(save_dir, exist_ok=True)\n    print(\"=\" * 80)\n    print(f\"Starting training at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(f\"Device: {device} | Epochs: {num_epochs} | Save dir: {save_dir}\")\n    print(\"=\" * 80)\n\n    train_losses, train_components_hist, epoch_times = [], [], []\n\n    for epoch in range(num_epochs):\n        t0 = time.time()\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\\n\" + \"-\" * 50)\n\n        tr_loss, tr_comps = train_maskrcnn_epoch(model, train_loader, optimizer, device)\n        train_losses.append(tr_loss)\n        train_components_hist.append(tr_comps)\n\n        if val_loader is not None:\n            val_loss, val_comps = validate_maskrcnn(model, val_loader, device)\n            print(f\"Val Loss: {val_loss:.4f}\")\n\n        epoch_time = time.time() - t0\n        epoch_times.append(epoch_time)\n        print(f\"Train Loss: {tr_loss:.4f} | Epoch time: {epoch_time:.1f}s\")\n\n        ckpt = {\n            \"epoch\": epoch + 1,\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n            \"train_loss\": tr_loss,\n        }\n        torch.save(ckpt, os.path.join(save_dir, f\"model_epoch_{epoch+1}.pth\"))\n        print(f\"âœ“ Saved checkpoint: epoch {epoch+1}\")\n\n    total_time = sum(epoch_times)\n    print(\"\\n\" + \"=\" * 80)\n    print(\"TRAINING COMPLETE\")\n    print(f\"Total time: {total_time:.1f}s | Avg/epoch: {np.mean(epoch_times):.1f}s\")\n    print(\"=\" * 80)\n\n    return {\n        \"train_losses\": train_losses,\n        \"train_loss_components\": train_components_hist,\n        \"training_time\": total_time,\n        \"avg_epoch_time\": float(np.mean(epoch_times) if epoch_times else 0.0),\n    }\n\n\ndef mask_iou(m1: np.ndarray, m2: np.ndarray) -> float:\n    inter = np.logical_and(m1, m2).sum()\n    union = np.logical_or(m1, m2).sum()\n    return float(inter / union) if union > 0 else 0.0\n\n\ndef evaluate_model(model, data_loader, device, iou_thresh=0.5, score_thresh=0.5, num_classes=NUM_CLASSES):\n    model.eval()\n\n    detection_records = []\n    per_class_ious = defaultdict(list)\n\n    with torch.no_grad():\n        for images, targets in data_loader:\n            images = [img.to(device) for img in images]\n            outputs = model(images)\n\n            for out, tgt in zip(outputs, targets):\n                p_boxes = out[\"boxes\"].cpu().numpy()\n                p_scores = out[\"scores\"].cpu().numpy()\n                p_labels = out[\"labels\"].cpu().numpy()\n                p_masks = out[\"masks\"].cpu().numpy()[:, 0] > 0.5 if \"masks\" in out else []\n\n                t_boxes = tgt[\"boxes\"].cpu().numpy()\n                t_labels = tgt[\"labels\"].cpu().numpy()\n                t_masks = tgt[\"masks\"].cpu().numpy() if \"masks\" in tgt else []\n\n                keep = p_scores >= score_thresh\n                p_boxes, p_scores, p_labels = p_boxes[keep], p_scores[keep], p_labels[keep]\n                p_masks = p_masks[keep] if len(p_masks) else []\n\n                used = set()\n                for pb, ps, pl, pm in zip(p_boxes, p_scores, p_labels, p_masks if len(p_masks) else [None] * len(p_boxes)):\n                    matched = False\n                    for j, (tb, tl) in enumerate(zip(t_boxes, t_labels)):\n                        if j in used or tl != pl:\n                            continue\n                        if len(t_masks):\n                            iou = mask_iou(pm, t_masks[j] > 0)\n                        else:\n                            xA = max(pb[0], tb[0]); yA = max(pb[1], tb[1])\n                            xB = min(pb[2], tb[2]); yB = min(pb[3], tb[3])\n                            inter = max(0, xB - xA) * max(0, yB - yA)\n                            area_p = (pb[2] - pb[0]) * (pb[3] - pb[1])\n                            area_t = (tb[2] - tb[0]) * (tb[3] - tb[1])\n                            union = area_p + area_t - inter\n                            iou = float(inter / union) if union > 0 else 0.0\n                        if iou >= iou_thresh:\n                            matched = True\n                            used.add(j)\n                            break\n                    detection_records.append({\"label\": int(pl), \"score\": float(ps), \"correct\": bool(matched)})\n\n                if len(p_masks) and len(t_masks):\n                    for tl in np.unique(t_labels):\n                        t_class_idxs = [k for k, lab in enumerate(t_labels) if lab == tl]\n                        if not t_class_idxs:\n                            continue\n                        for k in t_class_idxs:\n                            best = 0.0\n                            for pl2, pm2 in zip(p_labels, p_masks):\n                                if pl2 != tl:\n                                    continue\n                                best = max(best, mask_iou(pm2, t_masks[k] > 0))\n                            per_class_ious[int(tl)].append(best)\n\n    try:\n        from sklearn.metrics import average_precision_score\n\n        per_class_ap = {}\n        for c in range(1, num_classes):\n            recs = [r for r in detection_records if r[\"label\"] == c]\n            if not recs:\n                per_class_ap[c] = 0.0\n                continue\n            y_true = [1 if r[\"correct\"] else 0 for r in recs]\n            y_score = [r[\"score\"] for r in recs]\n            if len(set(y_true)) > 1:\n                per_class_ap[c] = float(average_precision_score(y_true, y_score))\n            else:\n                per_class_ap[c] = 0.0\n        mAP = float(np.mean(list(per_class_ap.values()))) if per_class_ap else 0.0\n    except Exception:\n        per_class_ap, mAP = {}, 0.0\n\n    per_class_mIoU = {c: (float(np.mean(v)) if len(v) else 0.0) for c, v in per_class_ious.items()}\n    return {\"mAP\": mAP, \"per_class_AP\": per_class_ap, \"per_class_mIoU\": per_class_mIoU}\n\n\ndef clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    valid_rows = []\n    for _, row in df.iterrows():\n        try:\n            if pd.isna(row[\"b_box\"]):\n                continue\n            x, y, w, h = (ast.literal_eval(row[\"b_box\"]) if isinstance(row[\"b_box\"], str) else row[\"b_box\"])[:4]\n            if w >= 3 and h >= 3:\n                valid_rows.append(row)\n        except Exception:\n            continue\n    return pd.DataFrame(valid_rows).reset_index(drop=True)\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ndef stratified_subsample(df, n_samples, label_col=\"category_name\"):\n    if n_samples >= len(df):\n        return df\n    subset, _ = train_test_split(df, train_size=n_samples, stratify=df[label_col], random_state=42)\n    return subset\n\n\ndef plot_category_distribution(df: pd.DataFrame, split_name: str = \"train\"):\n    if \"category_name\" not in df.columns:\n        print(f\"âŒ No category information available for {split_name} split\")\n        return\n    counts = df[\"category_name\"].value_counts()\n    plt.figure(figsize=(12, 6))\n    bars = plt.bar(range(len(counts)), counts.values, alpha=0.7)\n    plt.xlabel(\"Clothing Categories\")\n    plt.ylabel(\"Number of Samples\")\n    plt.title(f\"Category Distribution - {split_name.upper()}\")\n    plt.xticks(range(len(counts)), counts.index, rotation=45, ha=\"right\")\n    for bar, val in zip(bars, counts.values):\n        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 10, str(val), ha=\"center\", va=\"bottom\")\n    plt.tight_layout()\n    plt.show()\n\n\ndef show_image_grid(df, split='train', start_index=0, rows=2, cols=3):\n\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 10))\n    \n    if rows == 1 and cols == 1:\n        axes = [axes]\n    elif rows == 1 or cols == 1:\n        axes = axes.flatten()\n    else:\n        axes = axes.flatten()\n    \n    for i, ax in enumerate(axes):\n        index = start_index + i\n        \n        if index >= len(df):\n            ax.text(0.5, 0.5, 'No more images', ha='center', va='center', \n                   transform=ax.transAxes, fontsize=12)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            continue\n        \n        row = df.iloc[index]\n        \n        img = load_image(row['path'], split)\n        if img is None:\n            ax.text(0.5, 0.5, 'Image not found', ha='center', va='center', \n                   transform=ax.transAxes, fontsize=12)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            continue\n        \n        ax.imshow(img)\n        \n        if split != 'test':\n            if 'segmentation' in row and not pd.isna(row['segmentation']):\n                polygons = parse_segmentation(row['segmentation'])\n              \n                if polygons:\n                    mask = create_mask(polygons, img.shape)\n                    colored_mask = np.zeros_like(img)\n                    colored_mask[mask > 0] = [255, 100, 100]  \n                    ax.imshow(colored_mask, alpha=0.4)\n            \n            title_parts = []\n            if 'category_name' in row and not pd.isna(row['category_name']):\n                title_parts.append(f\"category_name: {row['category_name']}\")\n            \n            if 'scale' in row and not pd.isna(row['scale']):\n                title_parts.append(f\"Scale: {row['scale']}\")\n            \n            if 'occlusion' in row and not pd.isna(row['occlusion']):\n                title_parts.append(f\"Occl: {row['occlusion']}\")\n\n            if 'zoom_in' in row and not pd.isna(row['zoom_in']):\n                title_parts.append(f\"zoom_in: {row['zoom_in']}\")\n\n            if 'viewpoint' in row and not pd.isna(row['viewpoint']):\n                title_parts.append(f\"viewpoint: {row['viewpoint']}\")\n            \n            ax.set_title('\\n'.join(title_parts), fontsize=8)\n\n        ax.set_xticks([])\n        ax.set_yticks([])\n    \n    plt.tight_layout()\n    plt.suptitle(f'{split.upper()} Dataset', fontsize=14, y=1.02)\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    print(\"Loading datasets.\")\n    train_df = load_dataset(\"train\")\n    val_df = load_dataset(\"validation\")\n\n    print(\"Training samples grid:\")\n    show_image_grid(train_df, 'train', start_index=0, rows=5, cols=5)\n\n    print(\"Validation samples grid:\")\n    show_image_grid(val_df, 'validation', start_index=0, rows=5, cols=5)\n\n\n\n    \n\n    train_size = 300\n    val_size = 200\n\n    if train_size is not None:\n        print(f\"Subsampling train -> {train_size} samples\")\n        train_df = stratified_subsample(train_df, train_size)\n\n    if val_size is not None:\n        print(f\"Subsampling validation -> {val_size} samples\")\n        val_df = stratified_subsample(val_df, val_size)\n\n    if not train_df.empty:\n        train_df = clean_dataframe(train_df)\n\n    show_stats = False\n    if show_stats and not train_df.empty:\n        plot_category_distribution(train_df, \"train\")\n\n    train_set = DeepFashionMaskRCNNDataset(train_df, split=\"train\")\n    val_set = DeepFashionMaskRCNNDataset(val_df, split=\"validation\") if not val_df.empty else None\n\n    train_loader = DataLoader(train_set, batch_size=4, shuffle=True, num_workers=2, collate_fn=collate_fn)\n    val_loader = DataLoader(val_set, batch_size=4, shuffle=False, num_workers=2, collate_fn=collate_fn) if val_set else None\n\n\n    model = build_weighted_maskrcnn(NUM_CLASSES, CLASS_WEIGHTS, device=str(device))\n    model.to(device)\n\n    optimizer = torch.optim.SGD([p for p in model.parameters() if p.requires_grad], lr=0.005, momentum=0.9, weight_decay=0.0005)\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    train_history = train_model(\n        model,\n        train_loader=train_loader,\n        optimizer=optimizer,\n        device=device,\n        num_epochs=5,\n        save_dir=\"/kaggle/working/maskrcnn_training_results\",\n        val_loader=val_loader,\n    )\n\n    if val_loader is not None:\n        results = evaluate_model(model, val_loader, device, num_classes=NUM_CLASSES)\n        print(\"Evaluation:\", results)\n\n    print(\"ðŸŽ‰ Done.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score, confusion_matrix\nfrom collections import defaultdict\nimport torch\nimport time\nfrom datetime import datetime\nimport os\n\n\ndef validate_model(\n    model,\n    val_loader,\n    device,\n    save_dir=\"training_results\"\n):\n \n    print(\"=\" * 80)\n    print(f\"Starting validation at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(\"=\" * 80)\n    \n    validation_start_time = time.time()\n    \n    \n    val_loss, val_components = validate_maskrcnn(model, val_loader, device)\n    \n    validation_time = time.time() - validation_start_time\n    \n    print(f\"Validation Loss: {val_loss:.4f}\")\n    print(f\"Validation time: {format_time(validation_time)}\")\n    print(\"=\" * 80)\n    \n    return {\n        'val_loss': val_loss,\n        'val_loss_components': val_components,\n        'validation_time': validation_time\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes = [\n    \"background\",  \n    \"short sleeve top\",\n    \"trousers\",\n    \"shorts\",\n    \"long sleeve top\",\n    \"skirt\",\n    \"vest dress\",\n    \"short sleeve dress\",\n    \"vest\",\n    \"long sleeve outwear\",\n    \"long sleeve dress\",\n    \"sling dress\",\n    \"sling\",\n    \"short sleeve outwear\",\n]\ndef evaluate_model_comprehensive(\n    model,\n    data_loader,\n    device,\n    num_classes=14,\n    class_names=classes,\n    save_dir=\"training_results\",\n    dataset_name=\"validation\"\n):\n    \n    print(\"=\" * 80)\n    print(f\"Starting comprehensive evaluation at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(f\"Dataset: {dataset_name}\")\n    print(\"=\" * 80)\n    \n    evaluation_start_time = time.time()\n    \n    results = evaluate_model(model, data_loader, device, num_classes=num_classes)\n    \n    print(\"Generating confusion matrix...\")\n    y_true, y_pred = get_predictions_for_confusion_matrix(model, data_loader, device)\n    \n    if len(y_true) > 0 and len(y_pred) > 0:\n        plot_confusion_matrix(y_true, y_pred, class_names, save_dir)\n    \n    plot_metrics_summary(results, class_names, save_dir)\n    \n    evaluation_time = time.time() - evaluation_start_time\n    \n    print(f\"\\nEvaluation completed in: {format_time(evaluation_time)}\")\n    \n    results_summary = {\n        'evaluation_time': evaluation_time,\n        'metrics': results\n    }\n    \n    with open(f'{save_dir}/evaluation_summary_{dataset_name}.txt', 'w') as f:\n        f.write(f\"MASK R-CNN EVALUATION SUMMARY - {dataset_name.upper()}\\n\")\n        f.write(\"=\" * 50 + \"\\n\\n\")\n        f.write(f\"Evaluation completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        f.write(f\"Evaluation time: {format_time(evaluation_time)}\\n\")\n        f.write(f\"Number of classes: {num_classes}\\n\\n\")\n        \n        f.write(\"METRICS:\\n\")\n        f.write(\"-\" * 30 + \"\\n\")\n        for metric, value in results.items():\n            if isinstance(value, dict):\n                f.write(f\"{metric}:\\n\")\n                for k, v in value.items():\n                    if class_names and k < len(class_names):\n                        f.write(f\"  {class_names[k]}: {v:.4f}\\n\")\n                    else:\n                        f.write(f\"  {k}: {v:.4f}\\n\")\n            else:\n                f.write(f\"{metric}: {value:.4f}\\n\")\n    \n    print(f\"\\nEvaluation results saved to: {save_dir}\")\n    print(\"Files generated:\")\n    print(\"- confusion_matrix.png\")\n    print(\"- metrics_summary.png\")\n    print(f\"- evaluation_summary_{dataset_name}.txt\")\n    \n    return results_summary\n\ndef get_predictions_for_confusion_matrix(model, data_loader, device, score_thresh=0.5, iou_thresh=0.5):\n    model.eval()\n    all_pred_labels = []\n    all_true_labels = []\n    \n    with torch.no_grad():\n        for images, targets in data_loader:\n            images = [img.to(device) for img in images]\n            outputs = model(images)\n            \n            for output, target in zip(outputs, targets):\n                pred_scores = output[\"scores\"].cpu().numpy()\n                pred_labels = output[\"labels\"].cpu().numpy()\n                pred_boxes = output[\"boxes\"].cpu().numpy()\n                \n                keep = pred_scores >= score_thresh\n                if np.any(keep):\n                    pred_labels = pred_labels[keep]\n                    pred_boxes = pred_boxes[keep]\n                else:\n                    pred_labels = np.array([])\n                    pred_boxes = np.array([]).reshape(0, 4)\n                \n                gt_labels = target[\"labels\"].cpu().numpy()\n                gt_boxes = target[\"boxes\"].cpu().numpy()\n                \n                matched_preds, matched_gts = match_predictions_to_gt(\n                    pred_boxes, pred_labels, gt_boxes, gt_labels, iou_thresh\n                )\n                \n                all_pred_labels.extend(matched_preds)\n                all_true_labels.extend(matched_gts)\n    \n    return np.array(all_true_labels), np.array(all_pred_labels)\n\n\n\ndef match_predictions_to_gt(pred_boxes, pred_labels, gt_boxes, gt_labels, iou_thresh):\n    matched_preds = []\n    matched_gts = []\n    used_gt = set()\n    \n    for pred_box, pred_label in zip(pred_boxes, pred_labels):\n        best_iou = 0\n        best_gt_idx = -1\n        \n        for gt_idx, (gt_box, gt_label) in enumerate(zip(gt_boxes, gt_labels)):\n            if gt_idx in used_gt:\n                continue\n            \n            if pred_label == gt_label:\n                iou = calculate_box_iou(pred_box, gt_box)\n                if iou > best_iou and iou >= iou_thresh:\n                    best_iou = iou\n                    best_gt_idx = gt_idx\n        \n        if best_gt_idx != -1:\n            matched_preds.append(pred_label)\n            matched_gts.append(gt_labels[best_gt_idx])\n            used_gt.add(best_gt_idx)\n        else:\n            matched_preds.append(pred_label)\n            matched_gts.append(0)  \n    \n    for gt_idx, gt_label in enumerate(gt_labels):\n        if gt_idx not in used_gt:\n            matched_preds.append(0)  \n            matched_gts.append(gt_label)\n\n\n    return matched_preds, matched_gts\n\ndef plot_training_curves(train_losses, val_losses, train_maps, val_maps, train_loss_components, val_loss_components, save_dir=\"plots\"):\n    os.makedirs(save_dir, exist_ok=True)\n    \n    epochs = range(1, len(train_losses) + 1)\n    \n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    fig.suptitle('Training Progress', fontsize=16, fontweight='bold')\n    \n    axes[0, 0].plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2)\n    axes[0, 0].plot(epochs, val_losses, 'r-', label='Val Loss', linewidth=2)\n    axes[0, 0].set_title('Total Loss', fontweight='bold')\n    axes[0, 0].set_xlabel('Epoch')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    if train_maps and val_maps:\n        axes[0, 1].plot(epochs, train_maps, 'b-', label='Train mAP', linewidth=2)\n        axes[0, 1].plot(epochs, val_maps, 'r-', label='Val mAP', linewidth=2)\n        axes[0, 1].set_title('Mean Average Precision (mAP)', fontweight='bold')\n        axes[0, 1].set_xlabel('Epoch')\n        axes[0, 1].set_ylabel('mAP')\n        axes[0, 1].legend()\n        axes[0, 1].grid(True, alpha=0.3)\n    else:\n        axes[0, 1].text(0.5, 0.5, 'mAP data not available', \n                       ha='center', va='center', transform=axes[0, 1].transAxes)\n        axes[0, 1].set_title('Mean Average Precision (mAP)', fontweight='bold')\n    \n    train_cls = [comp['classifier'] for comp in train_loss_components]\n    val_cls = [comp['classifier'] for comp in val_loss_components]\n    axes[0, 2].plot(epochs, train_cls, 'b-', label='Train', linewidth=2)\n    axes[0, 2].plot(epochs, val_cls, 'r-', label='Val', linewidth=2)\n    axes[0, 2].set_title('Classification Loss', fontweight='bold')\n    axes[0, 2].set_xlabel('Epoch')\n    axes[0, 2].set_ylabel('Loss')\n    axes[0, 2].legend()\n    axes[0, 2].grid(True, alpha=0.3)\n    \n    train_box = [comp['box_reg'] for comp in train_loss_components]\n    val_box = [comp['box_reg'] for comp in val_loss_components]\n    axes[1, 0].plot(epochs, train_box, 'b-', label='Train', linewidth=2)\n    axes[1, 0].plot(epochs, val_box, 'r-', label='Val', linewidth=2)\n    axes[1, 0].set_title('Box Regression Loss', fontweight='bold')\n    axes[1, 0].set_xlabel('Epoch')\n    axes[1, 0].set_ylabel('Loss')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True, alpha=0.3)\n    \n    train_mask = [comp['mask'] for comp in train_loss_components]\n    val_mask = [comp['mask'] for comp in val_loss_components]\n    axes[1, 1].plot(epochs, train_mask, 'b-', label='Train', linewidth=2)\n    axes[1, 1].plot(epochs, val_mask, 'r-', label='Val', linewidth=2)\n    axes[1, 1].set_title('Mask Loss', fontweight='bold')\n    axes[1, 1].set_xlabel('Epoch')\n    axes[1, 1].set_ylabel('Loss')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True, alpha=0.3)\n    \n    train_obj = [comp['objectness'] for comp in train_loss_components]\n    val_obj = [comp['objectness'] for comp in val_loss_components]\n    train_rpn = [comp['rpn_box_reg'] for comp in train_loss_components]\n    val_rpn = [comp['rpn_box_reg'] for comp in val_loss_components]\n    \n    axes[1, 2].plot(epochs, train_obj, 'b-', label='Train Objectness', linewidth=2)\n    axes[1, 2].plot(epochs, val_obj, 'r-', label='Val Objectness', linewidth=2)\n    axes[1, 2].plot(epochs, train_rpn, 'b--', label='Train RPN Box', linewidth=2)\n    axes[1, 2].plot(epochs, val_rpn, 'r--', label='Val RPN Box', linewidth=2)\n    axes[1, 2].set_title('RPN Losses', fontweight='bold')\n    axes[1, 2].set_xlabel('Epoch')\n    axes[1, 2].set_ylabel('Loss')\n    axes[1, 2].legend()\n    axes[1, 2].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(f'{save_dir}/training_curves.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred, class_names=None, save_dir=\"plots\"):\n    os.makedirs(save_dir, exist_ok=True)\n    \n    cm = confusion_matrix(y_true, y_pred)\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names,\n                cbar_kws={'label': 'Count'})\n    plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n    plt.xlabel('Predicted Label', fontsize=12)\n    plt.ylabel('True Label', fontsize=12)\n    plt.tight_layout()\n    plt.savefig(f'{save_dir}/confusion_matrix.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\ndef plot_metrics_summary(results, class_names=None, save_dir=\"plots\"):\n    os.makedirs(save_dir, exist_ok=True)\n    \n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    fig.suptitle('Model Performance Metrics', fontsize=16, fontweight='bold')\n    \n    if 'detection_per_class_AP' in results and results['detection_per_class_AP']:\n        classes = list(results['detection_per_class_AP'].keys())\n        ap_values = list(results['detection_per_class_AP'].values())\n        \n        if class_names is not None:\n            class_labels = [class_names[c] if c < len(class_names) else f'Class {c}' for c in classes]\n        else:\n            class_labels = [f'Class {c}' for c in classes]\n        \n        axes[0, 0].bar(range(len(classes)), ap_values, alpha=0.7)\n        axes[0, 0].set_title('Per-Class AP (Detection)', fontweight='bold')\n        axes[0, 0].set_xlabel('Class')\n        axes[0, 0].set_ylabel('Average Precision')\n        axes[0, 0].set_xticks(range(len(classes)))\n        axes[0, 0].set_xticklabels(class_labels, rotation=45, ha='right')\n        axes[0, 0].grid(True, alpha=0.3)\n    \n    if 'per_class_IoU' in results and results['per_class_IoU']:\n        classes = list(results['per_class_IoU'].keys())\n        iou_values = list(results['per_class_IoU'].values())\n        \n        if class_names is not None:\n            class_labels = [class_names[c] if c < len(class_names) else f'Class {c}' for c in classes]\n        else:\n            class_labels = [f'Class {c}' for c in classes]\n        \n        axes[0, 1].bar(range(len(classes)), iou_values, alpha=0.7, color='orange')\n        axes[0, 1].set_title('Per-Class IoU', fontweight='bold')\n        axes[0, 1].set_xlabel('Class')\n        axes[0, 1].set_ylabel('IoU')\n        axes[0, 1].set_xticks(range(len(classes)))\n        axes[0, 1].set_xticklabels(class_labels, rotation=45, ha='right')\n        axes[0, 1].grid(True, alpha=0.3)\n    \n    metrics = ['precision', 'recall', 'f1', 'detection_mAP_050', 'segmentation_mAP_050', 'mIoU', 'pixel_accuracy']\n    metric_values = [results.get(m, 0) for m in metrics]\n    \n    axes[1, 0].bar(range(len(metrics)), metric_values, alpha=0.7, color='green')\n    axes[1, 0].set_title('Overall Performance Metrics', fontweight='bold')\n    axes[1, 0].set_xlabel('Metric')\n    axes[1, 0].set_ylabel('Score')\n    axes[1, 0].set_xticks(range(len(metrics)))\n    axes[1, 0].set_xticklabels(metrics, rotation=45, ha='right')\n    axes[1, 0].grid(True, alpha=0.3)\n    \n    if 'segmentation_per_class_AP' in results and results['segmentation_per_class_AP']:\n        det_classes = list(results['detection_per_class_AP'].keys())\n        seg_classes = list(results['segmentation_per_class_AP'].keys())\n        \n        common_classes = list(set(det_classes) & set(seg_classes))\n        \n        if common_classes:\n            det_ap = [results['detection_per_class_AP'][c] for c in common_classes]\n            seg_ap = [results['segmentation_per_class_AP'][c] for c in common_classes]\n            \n            if class_names is not None:\n                class_labels = [class_names[c] if c < len(class_names) else f'Class {c}' for c in common_classes]\n            else:\n                class_labels = [f'Class {c}' for c in common_classes]\n            \n            x = np.arange(len(common_classes))\n            width = 0.35\n            \n            axes[1, 1].bar(x - width/2, det_ap, width, label='Detection AP', alpha=0.7)\n            axes[1, 1].bar(x + width/2, seg_ap, width, label='Segmentation AP', alpha=0.7)\n            axes[1, 1].set_title('Detection vs Segmentation AP', fontweight='bold')\n            axes[1, 1].set_xlabel('Class')\n            axes[1, 1].set_ylabel('Average Precision')\n            axes[1, 1].set_xticks(x)\n            axes[1, 1].set_xticklabels(class_labels, rotation=45, ha='right')\n            axes[1, 1].legend()\n            axes[1, 1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(f'{save_dir}/metrics_summary.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\ndef format_time(seconds):\n    hours = int(seconds // 3600)\n    minutes = int((seconds % 3600) // 60)\n    seconds = int(seconds % 60)\n    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ncheckpoint_path = \"/kaggle/input/mask-rcnn-model/RCNN_model (1).pth\"\nmodel.load_state_dict(torch.load(checkpoint_path, map_location=\"cpu\"))\n\nmodel.eval()\n\nval_results = validate_model(\n    model=model,\n    val_loader=val_loader,\n    device=device,\n    save_dir=\"/kaggle/working/maskrcnn_training_results\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nevaluation_results = evaluate_model_comprehensive(\n    model=model,\n    data_loader=val_loader,\n    device=device,\n    num_classes=14,\n    class_names=classes,  \n    save_dir=\"/kaggle/working/maskrcnn_training_results\",\n    dataset_name=\"validation\"\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}